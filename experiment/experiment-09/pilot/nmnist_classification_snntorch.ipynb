{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "\n",
    "transform_original = tonic.transforms.Compose(\n",
    "    [\n",
    "        tonic.transforms.Denoise(filter_time=10000),\n",
    "        tonic.transforms.ToFrame(sensor_size=sensor_size, n_time_bins=30),\n",
    "        torch.from_numpy\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_noised = tonic.transforms.Compose(\n",
    "    [\n",
    "        tonic.transforms.Denoise(filter_time=10000),\n",
    "        tonic.transforms.UniformNoise(sensor_size=sensor_size, n=10),\n",
    "        tonic.transforms.ToFrame(sensor_size=sensor_size, n_time_bins=30),\n",
    "        torch.from_numpy\n",
    "    ]\n",
    ")\n",
    "\n",
    "situation = 'normal'\n",
    "\n",
    "if situation == 'normal':\n",
    "    train_dataset = tonic.datasets.NMNIST(save_to=\"/DATA/hwkang\", train=True, transform=transform_original)\n",
    "elif situation == 'noised':\n",
    "    train_dataset = tonic.datasets.NMNIST(save_to=\"/DATA/hwkang\", train=True, transform=transform_noised)\n",
    "\n",
    "test_dataset = tonic.datasets.NMNIST(save_to=\"/DATA/hwkang\", train=False, transform=transform_noised)\n",
    "\n",
    "cached_train_dataset = tonic.cached_dataset.MemoryCachedDataset(train_dataset)\n",
    "cached_test_dataset = tonic.cached_dataset.MemoryCachedDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "train_loader = DataLoader(cached_train_dataset, batch_size=100, num_workers=multiprocessing.cpu_count() // 2, shuffle=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(cached_test_dataset, batch_size=100, num_workers=multiprocessing.cpu_count() // 2, shuffle=False, collate_fn=tonic.collation.PadTensors(batch_first=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('experiment/experiment-09')\n",
    "from models import CNN\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(119)\n",
    "np.random.seed(119)\n",
    "random.seed(119)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "#model = SpikingCNN(beta, spike_grad)\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "#criterion = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_epochs = 3\n",
    "\n",
    "skip_train = False\n",
    "if skip_train is True:\n",
    "    model.load_state_dict(torch.load('/home/hwkang/jupyter/root/result/exp09/model/0000_exp09_model_cnn_original_0_0.pt'))\n",
    "if skip_train is False:\n",
    "\n",
    "    epoch_loss_rec = []\n",
    "    batch_loss_rec = []\n",
    "    min_loss = math.inf\n",
    "\n",
    "    min_epoch_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_loss = 0.0\n",
    "        current_step = 0\n",
    "        current_size = 0\n",
    "        for inputs, targets in train_loader: # >> [t, b, c, x, y] [b]\n",
    "            optimizer.zero_grad()\n",
    "            #outputs, _ = model(inputs.to(device)) # >> [t, b, num_neurons] \n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            current_step += 1\n",
    "            current_size += inputs.size(1)\n",
    "\n",
    "            if current_step % 100 == 0:\n",
    "                print(f'Epoch: {epoch}/{num_epochs} | Inputs: {current_size}/{len(train_loader.dataset)} | Batch Loss: {batch_loss / current_size:.6f}')\n",
    "\n",
    "            batch_loss_rec.append(batch_loss / current_size)\n",
    "\n",
    "        epoch_loss = batch_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs} | Epoch Loss: {epoch_loss:.6f}\\n')\n",
    "        epoch_loss_rec.append(epoch_loss)\n",
    "\n",
    "        if epoch_loss < min_epoch_loss:\n",
    "            min_epoch_loss = epoch_loss\n",
    "            PATH = 'experiment/experiment-09/weights'\n",
    "            filename = 'nmnist_snntorch_cnn_no_train_2.pt'\n",
    "            if skip_train is False:\n",
    "                import os\n",
    "                torch.save(model.state_dict(), os.path.join(PATH,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'experiment/experiment-09/weights'\n",
    "filename = 'nmnist_snntorch_cnn_noise_train.pt'\n",
    "if skip_train is False:\n",
    "    import os\n",
    "    torch.save(model.state_dict(), os.path.join(PATH,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "acc_metrics = torchmetrics.Accuracy(task='multiclass', num_classes=10).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    current_step = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    for inputs, targets in test_loader: # >> [b]\n",
    "        #outputs, _ = model(inputs.to(device)) # >> [t, b, num_neurons]\n",
    "        #batch_acc = SF.accuracy_rate(outputs, targets.to(device))\n",
    "        outputs = model(inputs.to(device))\n",
    "        predictions = torch.argmax(outputs, dim=1) # >> [b]\n",
    "        batch_acc = acc_metrics(predictions, targets.to(device))\n",
    "\n",
    "        current_step += 1\n",
    "        if current_step % 5 == 0:\n",
    "            print(f'Step: {current_step}/{len(test_loader)} | Batch Accuracy: {batch_acc * 100:.3f}%')\n",
    "\n",
    "        #all_predictions.append(outputs.detach().cpu())\n",
    "        all_predictions.append(predictions.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "\n",
    "    #all_predictions = torch.cat(all_predictions, dim=1)\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_accuracy = SF.accuracy_rate(all_predictions, all_targets)\n",
    "total_accuracy = acc_metrics(all_predictions, all_targets)\n",
    "\n",
    "print(f'Total Accuracy: {total_accuracy * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss trend\n",
    "epoch_indices = [599, 1199, 1799]\n",
    "epoch_loss_plot = [None] * len(batch_loss_rec)\n",
    "for idx, epoch_idx in enumerate(epoch_indices):\n",
    "    epoch_loss_plot[epoch_idx] = epoch_loss_rec[idx]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(batch_loss_rec)\n",
    "plt.plot(epoch_loss_plot, marker='x')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(test_loader))\n",
    "spk_out, mem_out = model(inputs.to(device)) # >> [t, b, num_neurons] [t, b, num_neurons]\n",
    "\n",
    "example_spk_train = spk_out[:, 0, :].detach().cpu() # << [t, num_neurons]\n",
    "example_mem_trend = mem_out[:, 0, :].detach().cpu() # << [t, num_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike count\n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
    "labels=['0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n",
    "print(f\"The target label is: {targets[0]}\")\n",
    "\n",
    "anim = splt.spike_count(example_spk_train, fig, ax, labels=labels, animate=True, interpolate=1)\n",
    "\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splt.traces(example_mem_trend, spk=example_spk_train, dim=(10,1), spk_height=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
