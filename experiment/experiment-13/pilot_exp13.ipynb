{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3fa46e-d2c7-4100-be54-885a64e6a87b",
   "metadata": {},
   "source": [
    "# Experiment no.13 \\[pilot\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b44c1-ebaa-4101-a0fc-2ca38926a310",
   "metadata": {},
   "source": [
    "# . Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5326bf-0a9e-461e-86d2-11786a92bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch family\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# torchvision family\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2 # G. model\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn # C. model\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# COCO family\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# utilities\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "from PIL import Image, ImageDraw # Visualization\n",
    "import json # Result management\n",
    "import preliminary.utils as utils\n",
    "sys.path.append(f'/home/hwkang/jupyter/root/experiment/experiment-13/preliminary')\n",
    "import preliminary.engine as engine\n",
    "\n",
    "# Stochastic perturbation\n",
    "sys.path.append('/home/hwkang/jupyter/root/')\n",
    "from utility.synthesize import generate_one_noisy_image\n",
    "from utility.preprocess import get_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02545c7-25ca-4a3a-a13c-962d8b9533f3",
   "metadata": {},
   "source": [
    "# . Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b1935-423c-46dd-b75a-6a59e4a39148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# Dataset\n",
    "path_root_coco = '/home/hwkang/jupyter/root/dataset/COCO2017/'\n",
    "path_train = os.path.join(path_root_coco, 'train2017')\n",
    "path_valid = os.path.join(path_root_coco, 'val2017')\n",
    "path_ann = os.path.join(path_root_coco, 'annotations')\n",
    "path_file_ann_train = os.path.join(path_ann, 'instances_train2017.json')\n",
    "path_file_ann_valid = os.path.join(path_ann, 'instances_val2017.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67c90b-1d99-4289-b221-c20c88f32354",
   "metadata": {},
   "source": [
    "# . Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a9cb5-c516-4ade-afc5-449188cb2e22",
   "metadata": {},
   "source": [
    "## .. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba5668-a177-49c0-92ca-52d27cc3d466",
   "metadata": {},
   "source": [
    "### ... Custom dataset declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378db7d-7777-4fde-a3a0-1a48d4865cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDetection(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, ann_file, transform=None):\n",
    "        self.root = image_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "        image_id = torch.tensor([img_id])\n",
    "        labels = [] # category_id \n",
    "        boxes = []\n",
    "        #areas = []\n",
    "        #iscrowds = []\n",
    "\n",
    "        for ann in anns:\n",
    "            #bbox_original = ann['bbox'] # CHECK\n",
    "            bbox = torch.tensor(ann['bbox'], dtype=torch.float32)\n",
    "            #bbox_before = bbox.clone() # CHECK\n",
    "            bbox[2:4] += bbox[0:2] # Convert format XYWH to XYXY\n",
    "\n",
    "            # If W and H are lesser equal than X_min and Y_min, then add tiny value\n",
    "            # 만약 W와 H가 offset(x,y)보다 작거나 같다면, 이 bbox 라벨에 아주 작은 값을 추가\n",
    "            if( bbox[0] >= bbox[2] or bbox[1] >= bbox[3] ):\n",
    "                if( bbox[0] >= bbox[2] ):\n",
    "                    bbox[2] += 0.1\n",
    "                if( bbox[1] >= bbox[3] ):\n",
    "                    bbox[3] += 0.1\n",
    "                #print(f'from_coco: {bbox_original}\\nbefore_convert: {bbox_before}\\nafter_convert: {bbox}\\n')\n",
    "            \n",
    "            labels.append(ann['category_id'])\n",
    "            boxes.append(bbox)\n",
    "            #areas.append(ann['areas'])\n",
    "            #iscrowds.append(ann['iscrowd']\n",
    "\n",
    "        # If it is not a background image which label is zero('0')\n",
    "        # 배경(background) 이미지가 아닌 경우\n",
    "        if len(boxes) > 0:\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            #boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            boxes = torch.stack(boxes)\n",
    "        \n",
    "        # Otherwise, that is background image\n",
    "        # 배경 이미지인 경우\n",
    "        else:\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # image_id: metadata\n",
    "        # labels: training, evaluation \n",
    "        # boxes: training, evaluation\n",
    "        # area: 'not in use' (LASTEST Upd.: 24-08-09 16:23)\n",
    "        # iscrowd: 'not in use' (LATEST Upd.: 24-08-09 16:23)\n",
    "        target = {\n",
    "            'image_id': image_id,\n",
    "            'labels': labels,\n",
    "            'boxes': boxes\n",
    "                 }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f249651-cd30-4bec-869a-0df00921a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictedLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, ann_file, results, transform=None):\n",
    "        self.root = image_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        \"\"\"\n",
    "        results contains ...\n",
    "        [image_id]: int\n",
    "        [category_id]: int\n",
    "        [bbox]: float\n",
    "        [score]: float\n",
    "        \"\"\"\n",
    "        self.results = results\n",
    "        self.ids = list({item['image_id'] for item in results}) # results에 있는 모든 image_id 리스트\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        # 실제 이미지 로딩\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "        image_id = torch.tensor([img_id])\n",
    "        labels = [] # category_id \n",
    "        boxes = []\n",
    "        areas = []\n",
    "        iscrowds = []\n",
    "\n",
    "        filtered_dicts = [d for d in self.results if d.get('image_id') == img_id]\n",
    "\n",
    "        for d in filtered_dicts:\n",
    "            bbox = torch.tensor(d['bbox'], dtype=torch.float32)\n",
    "            #bbox = np.array(d['bbox'])\n",
    "            bbox[2:4] += bbox[0:2] # Convert XYWH to XYXY\n",
    "\n",
    "            if( bbox[0] >= bbox[2] or bbox[1] >= bbox[3] ):\n",
    "                if( bbox[0] >= bbox[2] ):\n",
    "                    bbox[2] += 0.1\n",
    "                if( bbox[1] >= bbox[3] ):\n",
    "                    bbox[3] += 0.1\n",
    "                    \n",
    "            labels.append(d['category_id'])\n",
    "            boxes.append(bbox)\n",
    "\n",
    "        #boxes = np.array(boxes)\n",
    "\n",
    "        # If it is not a background image which label is zero('0')\n",
    "        # 배경(background) 이미지가 아닌 경우\n",
    "        if len(boxes) > 0:\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            #boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            boxes = torch.stack(boxes)\n",
    "        \n",
    "        # Otherwise, that is background image\n",
    "        # 배경 이미지인 경우\n",
    "        else:\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # image_id: metadata\n",
    "        # labels: training, evaluation \n",
    "        # boxes: training, evaluation\n",
    "        # area: 'not in use' (LASTEST Upd.: 24-08-09 16:23)\n",
    "        # iscrowd: 'not in use' (LATEST Upd.: 24-08-09 16:23)\n",
    "        target = {\n",
    "            'image_id': image_id,\n",
    "            'labels': labels,\n",
    "            'boxes': boxes,\n",
    "                 }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ae343-bf3d-4027-8469-c6f3c7ae5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76bbc5f-a19d-4a91-9108-838a2ef665b3",
   "metadata": {},
   "source": [
    "### ... Dataset instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc358c1-2ef1-43fb-b68f-d0a742f6b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train_dataset = CustomCocoDetection(path_train, path_file_ann_train, transform)\n",
    "\n",
    "# Split training dataset w. ratio '8:2'\n",
    "# 실제 훈련 데이터 9, 검증용 데이터 1로 기존 훈련용 데이터 분할\n",
    "dataset_size = len(train_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "valid_size = dataset_size - train_size\n",
    "\n",
    "# Split Training\n",
    "# Split Validation\n",
    "split_train_dataset, split_valid_dataset = random_split(train_dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee4ed9-a6f3-417c-9894-a4a27550a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample datasets '80%' to '10%' x 8 \n",
    "# Divide 'Split Training' to 'Sample Training'\n",
    "total_length = len(split_train_dataset)\n",
    "base_length = total_length // 8\n",
    "split_lengths = [base_length] * 8\n",
    "\n",
    "for i in range(total_length % 8):\n",
    "    split_lengths[i] += 1\n",
    "\n",
    "sample_datasets = random_split(split_train_dataset, split_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aeda92-7c5a-4641-b1a2-8096724f0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# NOTE: 원래는 검증용 데이터로 사용되어야 하나, 테스트 데이터가 없어 이 데이터셋을 테스트용으로 사용\n",
    "test_dataset = CustomCocoDetection(path_valid, path_file_ann_valid, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dfd34-9efe-4ace-9df3-d649f2a10308",
   "metadata": {},
   "source": [
    "## .. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c7915-ca63-457d-9880-6e62fa793934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances\n",
    "\n",
    "# Training\n",
    "#train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "# Split training\n",
    "#split_train_loader = DataLoader(split_train_dataset, batch_size=8, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "# Split Validation\n",
    "split_valid_loader = DataLoader(split_valid_dataset, batch_size=8, shuffle=False, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36216e-2e00-49ce-b0e8-031ec1e566fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training\n",
    "sample_loaders = [DataLoader(sample_dataset, batch_size=8, shuffle=True, collate_fn=utils.collate_fn) for sample_dataset in sample_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bfa5f-c45d-4d8e-a1eb-06412de8c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a56bb7-0aaa-42d3-8503-ef1038bb63f4",
   "metadata": {},
   "source": [
    "## .. CHECK phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99739e86-4605-4e0a-b62e-391b82228c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK: Custom dataset sanity\n",
    "def sanity_check(sanity_images, sanity_targets, flag_info=True, flag_image=False):\n",
    "    # Extract the first image and label from the batch\n",
    "    image = sanity_images[0]\n",
    "    target = sanity_targets[0]\n",
    "\n",
    "    # Check tensor validity\n",
    "    image_id = target['image_id']\n",
    "    labels = target['labels']\n",
    "    boxes = target['boxes']\n",
    "    denormed_boxes = target['denormed_boxes']\n",
    "    \n",
    "    info_data = f\"\"\"\n",
    "    image_id: {image_id}\\n\n",
    "    labels: {labels}\\n\n",
    "    boxes: {boxes}\\n\n",
    "    denormed_boxes: {denormed_boxes}\\n\n",
    "    min_image: {image.min()}, max_image: {image.max()}\\n\n",
    "    min_boxes: {boxes.min()}, max_boxes: {boxes.max()}\\n\n",
    "    min_denormed_boxes: {denormed_boxes.min()}, max_denormed_boxes: {denormed_boxes.max()}\n",
    "    \"\"\"\n",
    "    if( flag_info ):\n",
    "        print(info_data)\n",
    "    \n",
    "    if( flag_image):\n",
    "        # Plot image\n",
    "        permuted_image = torch.permute(input=image, dims=(1,2,0))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(permuted_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd5458-0da2-4709-9b7e-cbe0cd798a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK: data set length\n",
    "length_of_datasets = f\"\"\"\n",
    "The length of datasets\n",
    "train: {len(train_dataset)}\n",
    "split_train: {len(split_train_dataset)}\n",
    "split_valid: {len(split_valid_dataset)}\n",
    "test: {len(test_dataset)}\n",
    "\"\"\"\n",
    "print(length_of_datasets)\n",
    "\n",
    "# CHECK: sample dataset length\n",
    "# 각 데이터셋의 길이 확인\n",
    "for i, subset in enumerate(sample_datasets):\n",
    "    print(f\"Subset {i+1} length: {len(subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b8264-0ea4-47ca-8439-521b22caf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "#sanity_images, sanity_targets = next(iter(train_loader))\n",
    "#sanity_check(sanity_images, sanity_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43567701-c2d4-44b2-81c1-25e02d13f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "#sanity_images, sanity_targets = next(iter(test_loader))\n",
    "#sanity_check(sanity_images, sanity_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec50531-8b84-47f0-9c29-d3ac46f3aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK: Stochastic perturbed dataset sanity\n",
    "sanity_images, _ = next(iter(test_loader))\n",
    "\n",
    "samples = [sanity_images[i] for i in range(4)]\n",
    "noisy_samples = []\n",
    "\n",
    "for sample in samples:\n",
    "    # Inject stochastic perturbation\n",
    "    noisy_tensor = generate_one_noisy_image(sample, intensity=0.1, noise_type='gaussian')\n",
    "    \n",
    "    noisy_samples.append(noisy_tensor)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(noisy_samples[i].permute(1,2,0), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14a39-d3ee-4216-947f-fd07e042e137",
   "metadata": {},
   "source": [
    "# . Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ff1ac-4934-4e56-acff-5a415e63d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1b6dd-a634-4d2e-8044-9b136410f92a",
   "metadata": {},
   "source": [
    "## .. Golden model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd87a44-c8cb-4300-8380-73a5d3702e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G. model preparation\n",
    "# 골든 모델 준비\n",
    "\n",
    "G_model = fasterrcnn_resnet50_fpn_v2(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "G_model = G_model.to(device)                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb5b4d-f335-4557-a7b9-54212ce7fd60",
   "metadata": {},
   "source": [
    "### ... Label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb460f61-fe4e-461a-91d0-66f7c77239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_filtering(boxes, labels, scores, s_thr=0.5, n_thr=0.5):\n",
    "    # Score filtering\n",
    "    high_score_idxs = scores > s_thr\n",
    "    filtered_boxes = boxes[high_score_idxs]\n",
    "    filtered_scores = scores[high_score_idxs]\n",
    "    filtered_labels = labels[high_score_idxs]\n",
    "    \n",
    "    # NMS filtering\n",
    "    keep = nms(filtered_boxes, filtered_scores, n_thr)\n",
    "    nms_boxes = filtered_boxes[keep]\n",
    "    nms_scores = filtered_scores[keep]\n",
    "    nms_labels = filtered_labels[keep]\n",
    "\n",
    "    if len(keep) == 0:\n",
    "        return (False, None, None, None)\n",
    "    \n",
    "    return (True, nms_labels, nms_boxes, nms_scores)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec5ebb-7d1c-40a8-a788-678b4f71d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generation(model, data_loader, device, perturb=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    score_threshold = 0.5 # NOTE: 인자로 받을 것\n",
    "    nms_threshold = 0.5 # NOTE: 인자로 받을 것\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            # Inject noise\n",
    "            if( perturb ):\n",
    "                images = list(generate_one_noisy_image(image, intensity=0.8, noise_type='gaussian') for image in images)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "            # Predict\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Convert output to COCO evaluation format\n",
    "            for i, output in enumerate(outputs):\n",
    "                image_id = targets[i]['image_id'].item()\n",
    "                boxes = output['boxes']\n",
    "                labels = output['labels']\n",
    "                scores = output['scores']\n",
    "\n",
    "                # Convert bbox format from XYXY to XYWH\n",
    "                boxes[:, 2:] -= boxes[:, :2]\n",
    "\n",
    "                flag, labels, boxes, scores = label_filtering(\n",
    "                    boxes, labels, scores, score_threshold, nms_threshold)\n",
    "                if flag:\n",
    "                    for label, box, score in zip(labels, boxes, scores):\n",
    "                        result = {\n",
    "                            'image_id': int(image_id),\n",
    "                            'category_id': int(label),\n",
    "                            'bbox': box.tolist(),\n",
    "                            'score': float(score),\n",
    "                        }\n",
    "                        results.append(result)\n",
    "                else:\n",
    "                    continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541a2b-9057-4986-849a-fd02162c563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate label for L. m.\n",
    "#results_g = label_generation(G_model, sample_loaders[0], device, perturb=True)\n",
    "\n",
    "# Single dataset\n",
    "#predicted_label_dataset = PredictedLabelDataset(path_train, path_file_ann_train, results_g, transform) \n",
    "\n",
    "# Batch dataset\n",
    "predicted_label_datasets = [PredictedLabelDataset(path_train, path_file_ann_train,\n",
    "                                                  label_generation(G_model, sample_loader, device, perturb=True),\n",
    "                                                  transform) for sample_loader in sample_loaders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e11755-1a48-40ca-a740-1a302ae246df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single loader\n",
    "#predicted_label_loader = DataLoader(predicted_label_dataset, batch_size=8, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "# Batch loader\n",
    "predicted_label_loaders = [DataLoader(predicted_label_dataset, batch_size=8, shuffle=True, collate_fn=utils.collate_fn) for predicted_label_dataset in predicted_label_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe705c-a81d-4489-9b15-22a407dbc851",
   "metadata": {},
   "source": [
    "## .. Light-weight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64066b8b-a367-445d-bb29-de9eda9a52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L. model preparation\n",
    "# 경량 모델 준비\n",
    "\n",
    "L_model = fasterrcnn_mobilenet_v3_large_320_fpn(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "L_model = L_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc3ac2-968b-4cfc-ba5e-fcd14c2559f3",
   "metadata": {},
   "source": [
    "### ... Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adbf5a-95be-4b39-891b-f02c7d935fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters\n",
    "# 하이퍼파라미터 설정\n",
    "\n",
    "params = [p for p in L_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "num_epochs = 3\n",
    "\n",
    "# NOTE: 위 파라미터들은 정규 프로그램 구현 시 옵션 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2321dde-5a13-4b9b-9170-ce56cde6061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# 훈련 루프\n",
    "\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    engine.train_one_epoch(L_model, optimizer, sample_loaders[0], device, epoch, print_freq=500)\n",
    "    lr_scheduler.step()\n",
    "    #engine.evaluate(L_model, split_valid_loader, device=device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5942d2-661e-44dc-b6ef-0abdb15e4aa2",
   "metadata": {},
   "source": [
    "# . Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10311f-5a35-402d-9bf8-86b29d5b9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_evaluation(ann_file, results):\n",
    "    coco_gt = COCO(ann_file)\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "# NOTE: 유틸리티 라이브러리에 추가할 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbf881-5ca3-4b54-8950-8f90f733f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L. m. on test d.\n",
    "#results_l = label_generation(L_model, test_loader, device, perturb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e229ab-a1a3-4e3f-9f72-ccd755d756cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('result_l.json', 'w') as f:\n",
    "    json.dump(results_l, f)\n",
    "\n",
    "coco_evaluation(path_file_ann_valid, 'result_l.json')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776fd5d-9b05-4a0c-9c9e-a2710c1bd815",
   "metadata": {},
   "source": [
    "# . Continuous Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be4b61-6fa8-4fa1-9433-b9e806d45e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, predicted_label_loader in enumerate(predicted_label_loaders):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Step: [{step}] ##############################################')\n",
    "        print(f'########################################################')\n",
    "        \n",
    "        engine.train_one_epoch(L_model, optimizer, predicted_label_loader, device, epoch, print_freq=500)\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "        results_l = label_generation(L_model, test_loader, device, perturb=False)\n",
    "    \n",
    "        file_name = f'result_l_{step}.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(result_l, f)\n",
    "    \n",
    "        coco_evaluation(path_file_ann_valid, file_name)\n",
    "    \n",
    "        print(f'########################################################')\n",
    "        print(f'########################################################\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50cb1b-4d83-43b5-b71e-9358abf0aa14",
   "metadata": {},
   "source": [
    "# . Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c37eb-825c-4c69-aa02-a38fe8ee4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, color):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, outline=color, width=3)\n",
    "\n",
    "def show_image_with_boxes(image, pred_boxes, true_boxes, green=True, red=True):\n",
    "    image = transforms.ToPILImage()(image).convert(\"RGB\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    if red:\n",
    "        draw_boxes(image, pred_boxes, color='red')\n",
    "    if green:\n",
    "        draw_boxes(image, true_boxes, color='green')\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ebce0-6ea8-4de9-986a-35737ff82d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_image_with_boxes(image, None, de_boxes.tolist(), green=True, red=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
