{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3fa46e-d2c7-4100-be54-885a64e6a87b",
   "metadata": {},
   "source": [
    "# Experiment no.13 \\[pilot\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b44c1-ebaa-4101-a0fc-2ca38926a310",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5326bf-0a9e-461e-86d2-11786a92bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch family\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# torchvision family\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2 # G. model\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn # L. model\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# COCO family\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# utilities\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "from PIL import Image, ImageDraw # Visualization\n",
    "import json # Result management\n",
    "\n",
    "# Stochastic noise\n",
    "sys.path.append('/home/hwkang/jupyter/root/')\n",
    "sys.path.append('/home/hwkang/jupyter/root/utility/detection')\n",
    "from utility.synthesize import generate_one_noisy_image\n",
    "from utility.detection import utils, engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02545c7-25ca-4a3a-a13c-962d8b9533f3",
   "metadata": {},
   "source": [
    "# 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365b1935-423c-46dd-b75a-6a59e4a39148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "path_root_coco = '/home/hwkang/jupyter/root/dataset/COCO2017/'\n",
    "path_train = os.path.join(path_root_coco, 'train2017')\n",
    "path_valid = os.path.join(path_root_coco, 'val2017')\n",
    "path_ann = os.path.join(path_root_coco, 'annotations')\n",
    "path_file_ann_train = os.path.join(path_ann, 'instances_train2017.json')\n",
    "path_file_ann_valid = os.path.join(path_ann, 'instances_val2017.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67c90b-1d99-4289-b221-c20c88f32354",
   "metadata": {},
   "source": [
    "# 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a9cb5-c516-4ade-afc5-449188cb2e22",
   "metadata": {},
   "source": [
    "## 3.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba5668-a177-49c0-92ca-52d27cc3d466",
   "metadata": {},
   "source": [
    "### 3.1.1. Custom dataset declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a378db7d-7777-4fde-a3a0-1a48d4865cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDetection(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, ann_file, transform=None):\n",
    "        self.root = image_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "        image_id = torch.tensor([img_id])\n",
    "        labels = [] # category_id \n",
    "        boxes = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = torch.tensor(ann['bbox'], dtype=torch.float32)\n",
    "            bbox[2:4] += bbox[0:2] # Convert format XYWH to XYXY\n",
    "\n",
    "            # If W and H are lesser equal than X_min and Y_min, then add tiny value\n",
    "            # 만약 W와 H가 offset(x,y)보다 작거나 같다면, 이 bbox 라벨에 아주 작은 값을 추가\n",
    "            if( bbox[0] >= bbox[2] or bbox[1] >= bbox[3] ):\n",
    "                if( bbox[0] >= bbox[2] ):\n",
    "                    bbox[2] += 0.1\n",
    "                if( bbox[1] >= bbox[3] ):\n",
    "                    bbox[3] += 0.1\n",
    "            \n",
    "            labels.append(ann['category_id'])\n",
    "            boxes.append(bbox)\n",
    "\n",
    "        # If it is not a background image which label is zero('0')\n",
    "        # 배경(background) 이미지가 아닌 경우\n",
    "        if len(boxes) > 0:\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            boxes = torch.stack(boxes)\n",
    "        \n",
    "        # Otherwise, that is background image\n",
    "        # 배경 이미지인 경우\n",
    "        else:\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # image_id: metadata\n",
    "        # labels: training, evaluation \n",
    "        # boxes: training, evaluation\n",
    "        # area: 'not in use' (LASTEST Upd.: 24-08-09 16:23)\n",
    "        # iscrowd: 'not in use' (LATEST Upd.: 24-08-09 16:23)\n",
    "        target = {\n",
    "            'image_id': image_id,\n",
    "            'labels': labels,\n",
    "            'boxes': boxes\n",
    "                 }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f249651-cd30-4bec-869a-0df00921a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictedLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, ann_file, results, transform=None, noise=False):\n",
    "        self.root = image_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        \"\"\"\n",
    "        results contains ...\n",
    "        [image_id]: int\n",
    "        [category_id]: int\n",
    "        [bbox]: float\n",
    "        [score]: float\n",
    "        \"\"\"\n",
    "        self.results = results\n",
    "        self.ids = list({item['image_id'] for item in results}) # results에 있는 모든 image_id 리스트\n",
    "        self.transform = transform\n",
    "        self.noise = noise\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        # 실제 이미지 로딩\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "        image_id = torch.tensor([img_id])\n",
    "        labels = [] # category_id \n",
    "        boxes = []\n",
    "        areas = []\n",
    "        iscrowds = []\n",
    "\n",
    "        filtered_dicts = [d for d in self.results if d.get('image_id') == img_id]\n",
    "\n",
    "        for d in filtered_dicts:\n",
    "            bbox = torch.tensor(d['bbox'], dtype=torch.float32)\n",
    "            bbox[2:4] += bbox[0:2] # Convert XYWH to XYXY\n",
    "\n",
    "            if( bbox[0] >= bbox[2] or bbox[1] >= bbox[3] ):\n",
    "                if( bbox[0] >= bbox[2] ):\n",
    "                    bbox[2] += 0.1\n",
    "                if( bbox[1] >= bbox[3] ):\n",
    "                    bbox[3] += 0.1\n",
    "                    \n",
    "            labels.append(d['category_id'])\n",
    "            boxes.append(bbox)\n",
    "\n",
    "        # If it is not a background image which label is zero('0')\n",
    "        # 배경(background) 이미지가 아닌 경우\n",
    "        if len(boxes) > 0:\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            boxes = torch.stack(boxes)\n",
    "        \n",
    "        # Otherwise, that is background image\n",
    "        # 배경 이미지인 경우\n",
    "        else:\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.noise:\n",
    "            img = generate_one_noisy_image(img, intensity=0.5, noise_type='gaussian')\n",
    "\n",
    "        # image_id: metadata\n",
    "        # labels: training, evaluation \n",
    "        # boxes: training, evaluation\n",
    "        # area: 'not in use' (LASTEST Upd.: 24-08-09 16:23)\n",
    "        # iscrowd: 'not in use' (LATEST Upd.: 24-08-09 16:23)\n",
    "        target = {\n",
    "            'image_id': image_id,\n",
    "            'labels': labels,\n",
    "            'boxes': boxes,\n",
    "                 }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13ae343-bf3d-4027-8469-c6f3c7ae5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76bbc5f-a19d-4a91-9108-838a2ef665b3",
   "metadata": {},
   "source": [
    "### 3.1.2. Dataset instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9898f04b-0eea-4771-a20a-05f5dc00e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.70s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_dataset = CustomCocoDetection(path_train, path_file_ann_train, transform)\n",
    "\n",
    "# Resized training dataset\n",
    "train_dataset = Subset(train_dataset, list(range(10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48789c0c-423f-474f-8e60-10173978b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training dataset w. ratio '8:2'\n",
    "# 실제 훈련 데이터 9, 검증용 데이터 1로 기존 훈련용 데이터 분할\n",
    "dataset_size = len(train_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "valid_size = dataset_size - train_size\n",
    "\n",
    "# Split Training\n",
    "# Split Validation\n",
    "split_train_dataset, split_valid_dataset = random_split(train_dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ee4ed9-a6f3-417c-9894-a4a27550a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample datasets '80%' to '10%' x 8 \n",
    "# Divide 'Split Training' to 'Sample Training'\n",
    "total_length = len(split_train_dataset)\n",
    "base_length = total_length // 8\n",
    "split_lengths = [base_length] * 8\n",
    "\n",
    "for i in range(total_length % 8):\n",
    "    split_lengths[i] += 1\n",
    "\n",
    "sample_datasets = random_split(split_train_dataset, split_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5aeda92-7c5a-4641-b1a2-8096724f0867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.41s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# NOTE: 원래는 검증용 데이터로 사용되어야 하나, 테스트 데이터가 없어 이 데이터셋을 테스트용으로 사용\n",
    "test_dataset = CustomCocoDetection(path_valid, path_file_ann_valid, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dfd34-9efe-4ace-9df3-d649f2a10308",
   "metadata": {},
   "source": [
    "## 3.2. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1c7915-ca63-457d-9880-6e62fa793934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances\n",
    "\n",
    "# Split Validation\n",
    "split_valid_loader = DataLoader(split_valid_dataset, batch_size=8, shuffle=False, collate_fn=utils.collate_fn) # Not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df36216e-2e00-49ce-b0e8-031ec1e566fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training\n",
    "sample_loaders = [DataLoader(sample_dataset, batch_size=8, shuffle=True, collate_fn=utils.collate_fn) for sample_dataset in sample_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842bfa5f-c45d-4d8e-a1eb-06412de8c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a56bb7-0aaa-42d3-8503-ef1038bb63f4",
   "metadata": {},
   "source": [
    "## 3.2.1 CHECK phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99739e86-4605-4e0a-b62e-391b82228c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK: Custom dataset sanity\n",
    "def sanity_check(sanity_images, sanity_targets, flag_info=True, flag_image=False):\n",
    "    # Extract the first image and label from the batch\n",
    "    image = sanity_images[0]\n",
    "    target = sanity_targets[0]\n",
    "\n",
    "    # Check tensor validity\n",
    "    image_id = target['image_id']\n",
    "    labels = target['labels']\n",
    "    boxes = target['boxes']\n",
    "    denormed_boxes = target['denormed_boxes']\n",
    "    \n",
    "    info_data = f\"\"\"\n",
    "    image_id: {image_id}\\n\n",
    "    labels: {labels}\\n\n",
    "    boxes: {boxes}\\n\n",
    "    denormed_boxes: {denormed_boxes}\\n\n",
    "    min_image: {image.min()}, max_image: {image.max()}\\n\n",
    "    min_boxes: {boxes.min()}, max_boxes: {boxes.max()}\\n\n",
    "    min_denormed_boxes: {denormed_boxes.min()}, max_denormed_boxes: {denormed_boxes.max()}\n",
    "    \"\"\"\n",
    "    if( flag_info ):\n",
    "        print(info_data)\n",
    "    \n",
    "    if( flag_image):\n",
    "        # Plot image\n",
    "        permuted_image = torch.permute(input=image, dims=(1,2,0))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(permuted_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14a39-d3ee-4216-947f-fd07e042e137",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ff1ac-4934-4e56-acff-5a415e63d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1b6dd-a634-4d2e-8044-9b136410f92a",
   "metadata": {},
   "source": [
    "## 4.1. Golden model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd87a44-c8cb-4300-8380-73a5d3702e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G. model preparation\n",
    "# 골든 모델 준비\n",
    "\n",
    "G_model = fasterrcnn_resnet50_fpn_v2(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "G_model = G_model.to(device)                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb5b4d-f335-4557-a7b9-54212ce7fd60",
   "metadata": {},
   "source": [
    "### 4.1.1. Label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb460f61-fe4e-461a-91d0-66f7c77239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_filtering(boxes, labels, scores, s_thr=0.5, n_thr=0.5):\n",
    "    # Score filtering\n",
    "    high_score_idxs = scores > s_thr\n",
    "    filtered_boxes = boxes[high_score_idxs]\n",
    "    filtered_scores = scores[high_score_idxs]\n",
    "    filtered_labels = labels[high_score_idxs]\n",
    "    \n",
    "    # NMS filtering\n",
    "    keep = nms(filtered_boxes, filtered_scores, n_thr)\n",
    "    nms_boxes = filtered_boxes[keep]\n",
    "    nms_scores = filtered_scores[keep]\n",
    "    nms_labels = filtered_labels[keep]\n",
    "\n",
    "    if len(keep) == 0:\n",
    "        return (False, None, None, None)\n",
    "    \n",
    "    return (True, nms_labels, nms_boxes, nms_scores)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec5ebb-7d1c-40a8-a788-678b4f71d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generation(model, data_loader, device, noise=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    score_threshold = 0.5 # NOTE: 인자로 받을 것\n",
    "    nms_threshold = 0.5 # NOTE: 인자로 받을 것\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            # Inject noise\n",
    "            if( noise ):\n",
    "                images = list(generate_one_noisy_image(image, intensity=0.5, noise_type='gaussian') for image in images)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "            # Predict\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Convert output to COCO evaluation format\n",
    "            for i, output in enumerate(outputs):\n",
    "                image_id = targets[i]['image_id'].item()\n",
    "                boxes = output['boxes']\n",
    "                labels = output['labels']\n",
    "                scores = output['scores']\n",
    "\n",
    "                # Convert bbox format from XYXY to XYWH\n",
    "                boxes[:, 2:] -= boxes[:, :2]\n",
    "\n",
    "                flag, labels, boxes, scores = label_filtering(\n",
    "                    boxes, labels, scores, score_threshold, nms_threshold)\n",
    "                if flag:\n",
    "                    for label, box, score in zip(labels, boxes, scores):\n",
    "                        result = {\n",
    "                            'image_id': int(image_id),\n",
    "                            'category_id': int(label),\n",
    "                            'bbox': box.tolist(),\n",
    "                            'score': float(score),\n",
    "                        }\n",
    "                        results.append(result)\n",
    "                else:\n",
    "                    continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541a2b-9057-4986-849a-fd02162c563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate label for L. m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498b359-c484-4ec0-ade6-506cdf490c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "# (Image: noise, Label: noise)\n",
    "nn_datasets = [PredictedLabelDataset(path_train, path_file_ann_train,\n",
    "                                    label_generation(G_model, sample_loader, device, noise=True),\n",
    "                                    transform, noise=True) for sample_loader in sample_loaders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcf323-308c-4d8a-a8b3-0422fb0004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: noise, Label: clean)\n",
    "nc_datasets = [PredictedLabelDataset(path_train, path_file_ann_train,\n",
    "                                    label_generation(G_model, sample_loader, device, noise=False),\n",
    "                                    transform, noise=True) for sample_loader in sample_loaders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dde6fd-7813-4b21-a8ac-3437fd809c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: clean, Label: noise)\n",
    "cn_datasets = [PredictedLabelDataset(path_train, path_file_ann_train,\n",
    "                                    label_generation(G_model, sample_loader, device, noise=True),\n",
    "                                    transform, noise=False) for sample_loader in sample_loaders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b1903-690a-4119-bd1e-0187bbc4f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: clean, Label: clean)\n",
    "cc_datasets = [PredictedLabelDataset(path_train, path_file_ann_train,\n",
    "                                    label_generation(G_model, sample_loader, device, noise=False),\n",
    "                                    transform, noise=False) for sample_loader in sample_loaders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d064272-f352-4254-81bc-81a84839d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_size(datasets):\n",
    "    total = 0\n",
    "    for dataset in datasets:\n",
    "        total += len(dataset)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68a288-3f99-44be-8dbf-a03205dab046",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_nn = get_dataset_size(nn_datasets)\n",
    "size_nc = get_dataset_size(nc_datasets)\n",
    "size_cn = get_dataset_size(cn_datasets)\n",
    "size_cc = get_dataset_size(cc_datasets)\n",
    "print(f'Size of\\nNN: {size_nn}\\nNC: {size_nc}\\nCN: {size_cn}\\nCC: {size_cc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612a510-59fa-4b70-96b3-3a217d4e0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a319974-600f-42b3-be2a-e43cb7c06003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: noise, label: noise)\n",
    "nn_loaders = [DataLoader(nn_dataset, batch_size=8, shuffle=True,\n",
    "                                           collate_fn=utils.collate_fn) for nn_dataset in nn_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88346a58-ad66-479e-b969-5eca8446ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: noise, label: clean)\n",
    "nc_loaders = [DataLoader(nc_dataset, batch_size=8, shuffle=True,\n",
    "                                           collate_fn=utils.collate_fn) for nc_dataset in nc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee40d92-ae90-431c-a822-c1d2fcbc2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: clean, label: noise)\n",
    "cn_loaders = [DataLoader(cn_dataset, batch_size=8, shuffle=True,\n",
    "                                           collate_fn=utils.collate_fn) for cn_dataset in cn_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50debe-c89d-40b2-904e-ef537114cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Image: clean, label: clean)\n",
    "cc_loaders = [DataLoader(cc_dataset, batch_size=8, shuffle=True,\n",
    "                                           collate_fn=utils.collate_fn) for cc_dataset in cc_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe705c-a81d-4489-9b15-22a407dbc851",
   "metadata": {},
   "source": [
    "## 4.2. Light-weight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64066b8b-a367-445d-bb29-de9eda9a52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L. model preparation\n",
    "# 경량 모델 준비\n",
    "\n",
    "# No Retrainig(Baseline)\n",
    "L_model = fasterrcnn_mobilenet_v3_large_320_fpn(weights='DEFAULT',\n",
    "                                   num_classes=91)\n",
    "L_model = L_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fedcc-8baf-4915-a716-048784bfc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_model_NN = fasterrcnn_mobilenet_v3_large_320_fpn(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "L_model_NN = L_model_NN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2882b-6598-4949-a7d4-6927fb3114cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_model_NC = fasterrcnn_mobilenet_v3_large_320_fpn(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "L_model_NC = L_model_NC.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075cce8-2062-4ad0-8a55-b1e6c5b37af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_model_CN = fasterrcnn_mobilenet_v3_large_320_fpn(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "L_model_CN = L_model_CN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598131fb-8b5e-479b-ad32-8938beeabf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_model_CC = fasterrcnn_mobilenet_v3_large_320_fpn(weights='COCO_V1',\n",
    "                                   num_classes=91)\n",
    "L_model_CC = L_model_CC.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc3ac2-968b-4cfc-ba5e-fcd14c2559f3",
   "metadata": {},
   "source": [
    "### 4.2.1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918a9fb-3014-4586-9796-6dfe6009f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acf7aa-b5dd-45f1-834c-fb839a0ca71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in L_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adbf5a-95be-4b39-891b-f02c7d935fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters\n",
    "# 하이퍼파라미터 설정\n",
    "\n",
    "params_NN = [p for p in L_model_NN.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params_NN,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# NOTE: 위 파라미터들은 정규 프로그램 구현 시 옵션 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef54590-9523-4422-9fe7-3451a3877642",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_NC = [p for p in L_model_NC.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params_NC,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452142f-a342-4d9b-b54c-b98f14e88728",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_CN = [p for p in L_model_CN.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params_CN,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4c3a0-0499-41f0-ae59-ed35841ff9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_CC = [p for p in L_model_CC.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params_CC,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5942d2-661e-44dc-b6ef-0abdb15e4aa2",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10311f-5a35-402d-9bf8-86b29d5b9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_evaluation(ann_file, results):\n",
    "    coco_gt = COCO(ann_file)\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "# NOTE: 유틸리티 라이브러리에 추가할 것!\n",
    "# NOTE: evaluate(v1.0.1)에 추가됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776fd5d-9b05-4a0c-9c9e-a2710c1bd815",
   "metadata": {},
   "source": [
    "# 6. Continuous Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11faf0-23a8-4ee9-b7ac-138a2277c6fc",
   "metadata": {},
   "source": [
    "## 6.1. 노이즈 발생 위치에 따른 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc9464-7467-46b8-94ea-0b60178667a4",
   "metadata": {},
   "source": [
    "### 6.1.1. Image: Noise, Label: Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7326f-bda3-492e-970d-237fbae467dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, predicted_label_loader in enumerate(nn_loaders):\n",
    "    print(f'Step: [{step}] ##############################################')\n",
    "    print(f'########################################################')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        engine.train_one_epoch(L_model_NN, optimizer, predicted_label_loader, device, epoch, print_freq=50)\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    results_NN = label_generation(L_model_NN, test_loader, device, noise=False)\n",
    "\n",
    "    file_name = f'result_NN_{step}.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(results_NN, f)\n",
    "\n",
    "    coco_evaluation(path_file_ann_valid, file_name)\n",
    "    \n",
    "    print(f'########################################################')\n",
    "    print(f'########################################################\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc2293-eedd-434a-8844-c545a6b204e6",
   "metadata": {},
   "source": [
    "### 6.1.2. Image: Noise, Label: Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236dfff1-b228-4dc1-be5e-75980c89c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, predicted_label_loader in enumerate(nc_loaders):\n",
    "    print(f'Step: [{step}] ##############################################')\n",
    "    print(f'########################################################')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        engine.train_one_epoch(L_model_NC, optimizer, predicted_label_loader, device, epoch, print_freq=50)\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    results_NC = label_generation(L_model_NC, test_loader, device, noise=False)\n",
    "\n",
    "    file_name = f'result_NC_{step}.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(results_NC, f)\n",
    "\n",
    "    coco_evaluation(path_file_ann_valid, file_name)\n",
    "    \n",
    "    print(f'########################################################')\n",
    "    print(f'########################################################\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbb688-d5bd-4b74-b25d-492e386ca287",
   "metadata": {},
   "source": [
    "### 6.1.3. Image: Clean, Label: Noise (DONE) - Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee81dc0-e647-4016-93e5-100754701741",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, predicted_label_loader in enumerate(cn_loaders):\n",
    "    print(f'Step: [{step}] ##############################################')\n",
    "    print(f'########################################################')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        engine.train_one_epoch(L_model_CN, optimizer, predicted_label_loader, device, epoch, print_freq=50)\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    results_CN = label_generation(L_model_CN, test_loader, device, noise=False)\n",
    "\n",
    "    file_name = f'result_CN_{step}.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(results_CN, f)\n",
    "\n",
    "    coco_evaluation(path_file_ann_valid, file_name)\n",
    "    \n",
    "    print(f'########################################################')\n",
    "    print(f'########################################################\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6eafa-8846-40fc-a431-328943172f0c",
   "metadata": {},
   "source": [
    "### 6.1.4. Image: Clean, Label: Clean (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cea86-b7b1-4c6f-8ba2-8fcb64af57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, predicted_label_loader in enumerate(cc_loaders):\n",
    "    print(f'Step: [{step}] ##############################################')\n",
    "    print(f'########################################################')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        engine.train_one_epoch(L_model_CC, optimizer, predicted_label_loader, device, epoch, print_freq=50)\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    results_CC = label_generation(L_model_CC, test_loader, device, noise=False)\n",
    "\n",
    "    file_name = f'result_CC_{step}.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(results_CC, f)\n",
    "\n",
    "    coco_evaluation(path_file_ann_valid, file_name)\n",
    "    \n",
    "    print(f'########################################################')\n",
    "    print(f'########################################################\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50cb1b-4d83-43b5-b71e-9358abf0aa14",
   "metadata": {},
   "source": [
    "# 7. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0a792-7cb5-4f55-9b1a-01037e01db71",
   "metadata": {},
   "source": [
    "## 7.1. 잡음 발생 위치에 따른 재훈련 시 Light-weight model 객체 탐지 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bb0e2-2797-4426-b9b9-4e0fa0d8432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "NN = [0.121, 0.133] + [0.134] * 6\n",
    "NC = [0.117, 0.133] + [0.134] * 6\n",
    "CN = [0.073, 0.06] + [0.059] * 6\n",
    "CC = [0.188, 0.195] + [0.196] * 6\n",
    "NR = [0.2] * 8\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(NR, label='No Retraining(Default)', marker='D')\n",
    "plt.plot(CC, label='Clean Image, Clean Label', marker='*')\n",
    "plt.plot(NN, label='Noised Image, Noised Label', marker='o')\n",
    "plt.plot(NC, label='Noised Image, Clean Label', marker='x')\n",
    "plt.plot(CN, label='Clean Image, Noised Label', marker='s')\n",
    "\n",
    "plt.title('mAP on Retraining Dataset Noise')\n",
    "plt.xlabel('Retraining Window')\n",
    "plt.ylabel('mAP')\n",
    "plt.legend(loc='center', bbox_to_anchor=(0.8,0.3))\n",
    "plt.grid(False)\n",
    "\n",
    "plt.savefig('mAP_on_Retraining_Dataset_Noise.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10c6d1-0cb3-47f5-9877-c2e2121dd39b",
   "metadata": {},
   "source": [
    "## 7.2. 잡음 발생 위치에 따른 Golden model의 예측을 통한 생성 라벨의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacf2ef-48c8-4bc5-8d4a-09228a624966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주와 값\n",
    "# Results\n",
    "categories = ['No Retraining\\n(Default)', 'Clean Image\\nClean Label', 'Noised Image\\nNoise Label', 'Noised Image\\nClean Label', 'Clean Image\\nNoised Label']\n",
    "values = [8000, 7963, 4147, 7963, 4101]\n",
    "\n",
    "# 막대그래프를 그리기 위한 준비\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "# 막대그래프 생성\n",
    "bars = plt.bar(categories, values, color=colors)\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()  # 막대의 높이(값)를 가져옴\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval/2, int(yval), ha='center', va='bottom')\n",
    "\n",
    "# 그래프의 제목과 라벨을 설정\n",
    "plt.title('Dataset Size on Noise Existence')\n",
    "plt.ylabel('Dataset Size')\n",
    "\n",
    "# 그래프를 화면에 표시\n",
    "plt.savefig('Dataset_Size_on_Noise_Existence')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
