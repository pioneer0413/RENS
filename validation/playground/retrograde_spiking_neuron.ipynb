{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import spikegen\n",
    "from snntorch import functional as SF\n",
    "\n",
    "import snntorch.spikeplot as splt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('utility')\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# 데이터셋에 대한 변환 정의 (이미지를 텐서로 변환하고, 784차원 벡터로 펼침)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # 28x28 이미지를 784 차원 벡터로 변환\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋 다운로드 및 로드\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader를 통해 배치로 데이터를 로드\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # FC - Leaky - FC - Leaky\n",
    "        self.fc1 = nn.Linear(784, 1000)\n",
    "        self.lif1 = snn.Leaky(beta=0.5)\n",
    "        self.fc2 = nn.Linear(1000,10)\n",
    "        self.lif2 = snn.Leaky(beta=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky().cuda()\n",
    "        mem2 = self.lif2.init_leaky().cuda()\n",
    "\n",
    "        num_steps = x.size(0) # << [num_steps, 784]\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        for step in range(num_steps):\n",
    "            \n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "        \n",
    "        return torch.stack(spk1_rec), torch.stack(mem1_rec), torch.stack(spk2_rec), torch.stack(mem2_rec) # num_steps * [4] >> [num_steps, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrogradeSNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # FC - Leaky - FC - Leaky\n",
    "        self.fc1 = nn.Linear(784,1000)\n",
    "        self.lif1 = snn.Leaky(beta=0.5)\n",
    "        self.fc2 = nn.Linear(1000,10)\n",
    "        self.lif2 = snn.Leaky(beta=0.5)\n",
    "        self.retrograde_fc = nn.Linear(10, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        #retrograde_spk1 = torch.zeros_like(x)\n",
    "        retrograde_spk2 = torch.zeros(1000) # [1000]\n",
    "\n",
    "        num_steps = x.size(0) # << [num_steps, 4]\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step]) # [784] >> [1000]\n",
    "            spk1, mem1 = self.lif1(cur1, mem1) # [1000] >> [1000]\n",
    "            \n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "            \n",
    "            cur2 = self.fc2(spk1) # [1000] >> [10]\n",
    "            spk2, mem2 = self.lif2(cur2, mem2) # [10] >> [10]\n",
    "\n",
    "\n",
    "\n",
    "            retrograde_spk2 = torch.clone(spk2) \n",
    "\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "        \n",
    "        return torch.stack(spk1_rec), torch.stack(mem1_rec), torch.stack(spk2_rec), torch.stack(mem2_rec) # num_steps * [4] >> [num_steps, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ann(model, train_loader, criterion, optimizer, epochs=3):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "            # 옵티마이저 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 역전파 및 옵티마이저 스텝\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # 에포크마다 평균 손실 출력\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "def evaluate_ann(model, test_loader):\n",
    "    model.eval()  # 평가 모드로 전환\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 평가 시에는 그래디언트를 계산하지 않음\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 가장 높은 확률의 클래스 예측\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ANN().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ann.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] | Loss: 0.2550\n"
     ]
    }
   ],
   "source": [
    "train_ann(ann, train_loader=train_loader, criterion=criterion, optimizer=optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.66%\n"
     ]
    }
   ],
   "source": [
    "evaluate_ann(ann, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn(model, train_loader, criterion, optimizer, epochs=3):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "            spk_in = spikegen.rate(images, 8) # >> [num_steps, batch_size, 784]\n",
    "\n",
    "            # 옵티마이저 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 순전파\n",
    "            _, _, outputs, _ = model(spk_in)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 역전파 및 옵티마이저 스텝\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # 에포크마다 평균 손실 출력\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "def evaluate_snn(model, test_loader):\n",
    "    model.eval()  # 평가 모드로 전환\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # 평가 시에는 그래디언트를 계산하지 않음\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            spk_in = spikegen.rate(images, 8)\n",
    "\n",
    "            _, _, outputs, _ = model(spk_in) # >> [num_steps, batch_size, num_neurons]\n",
    "            \n",
    "            all_predictions.append(outputs) # >> num_batches * [num_steps, batch_size, num_neurons]\n",
    "            all_labels.append(labels) # >> num_batches * [batch_size]\n",
    "\n",
    "    all_predictions = torch.concat(all_predictions, dim=1)\n",
    "    all_labels = torch.concat(all_labels, dim=0)\n",
    "    \n",
    "    accuracy = SF.accuracy_rate(all_predictions, all_labels)\n",
    "    print(f'Accuracy on test set: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spikes(spk_in):\n",
    "    for idx, neuron in enumerate(torch.split(spk_in, 1, dim=1)):\n",
    "        print(f'#{idx}: {len(neuron.nonzero())}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_snn = SNN().cuda()\n",
    "criterion = SF.ce_rate_loss()\n",
    "optimizer = torch.optim.Adam(simple_snn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] | Loss: 1.5643\n"
     ]
    }
   ],
   "source": [
    "train_snn(simple_snn, train_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 94.08%\n"
     ]
    }
   ],
   "source": [
    "evaluate_snn(simple_snn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrograde_snn = RetrogradeSNN().cuda()\n",
    "criterion = SF.ce_rate_loss()\n",
    "optimizer = torch.optim.Adam(retrograde_snn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snn(retrograde_snn, train_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_snn(retrograde_snn, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
